{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Logistic Regression Model\n",
    "\n",
    "# Datascience DIKU 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook details our work on our Logistic Regression model. The variable model_list can be swapped out, such that it trains random forest, gradient boosting, multinomial naive bayes, ada boost and decision tree classifier.\n",
    "\n",
    "## User guide\n",
    "The notebook requires the LoadingAndCleaning.py file, as well as data from FakeNewsCorpus and some scraped wikinews, which can be turned off. We also use data from the LIAR set, and a Kaggle competition to get predictions after our training. With the right data, the notebook can be run from start to finish, using some booleans at the start to control important parameters.\n",
    "\n",
    "## Cleaning\n",
    " The support py module does various cleaning. Importantly, it drops \"unknown\" types, and articles longer than 24000 chars. It maps \"reliable\" and \"political\" to genuine news and all else to fake. It includes lowering, contraction expansion, generic whitespace truncation and removal of odd characters. We also replace emails, dates, urls and nums with e.g. NUMNUM. We tokenize and do stopword removal as well, before recombining our content to a processed string for TF IDF fitting and transform. We also append some scraped wikinews we have, which can be turned off.\n",
    "\n",
    "## Training\n",
    "We do a default 25/75 train/test split. We use optimized C and max iter values for Log. reg., see the bottom of the notebook for sklearns gridsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from LoadingAndCleaning import init_dataframe\n",
    "import LoadingAndCleaning\n",
    "import Vizualisations\n",
    "import LoadingAndCleaningNoMultiProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_wikinews():\n",
    "    wiki_df = pd.read_csv('data/wiki_data.csv', index_col=[0])\n",
    "    wiki_df['url'] = 0\n",
    "    wiki_df['scraped_at'] = 0\n",
    "    wiki_df['type'] = 1\n",
    "    wiki_df['domain'] = 0\n",
    "    wiki_df['authors'] = \"\"\n",
    "    wiki_df['content'] = wiki_df['content'].astype('str')\n",
    "    wiki_df = LoadingAndCleaning.trim_df_only_content(wiki_df)\n",
    "    wiki_df = LoadingAndCleaning.clean_df_only_content(wiki_df)\n",
    "    wiki_df['label'] = 1\n",
    "    wiki_df = LoadingAndCleaning.process_df_only_content(wiki_df)\n",
    "    \n",
    "    return wiki_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_df_labels_remove_fake(df): \n",
    "    df_real = df[df['label'] == 1]\n",
    "    df_fake = df[df['label'] == 0]\n",
    "    min_len = len(df_real)\n",
    "    df_adjusted = df_real.append(df_fake.sample(n=min_len))\n",
    "    \n",
    "    return df_adjusted\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 If only using content, for Kaggle and LIAR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = 'data/1mio-raw.csv'\n",
    "percent_load = 0.1\n",
    "use_multiprocessing = False\n",
    "is_clean_authors_and_title = False # cleaned and processed like content, ie tokenized\n",
    "use_wiki = True\n",
    "\n",
    "save_kaggle_submission = True\n",
    "kaggle_path = 'data/submissions/base_model_preds.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Initialize, trim, clean, process and balance label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataframe...\n",
      "Initialized data (100594 entries) in: 5.8 seconds\n",
      "Cleaning...\n",
      "Processing...\n",
      "Processed(66036 entries) in: 85.1 seconds\n",
      "Cleaning...\n",
      "Cleaned (3323 entries) in: 1.9 seconds\n",
      "Processing...\n",
      "Processed(3323 entries) in: 3.2 seconds\n"
     ]
    }
   ],
   "source": [
    "# Init and trim dataframe\n",
    "df = init_dataframe(load_path, percent_load)\n",
    "df['content'] = df['content'].astype('str')\n",
    "df = LoadingAndCleaning.trim_df_only_content(df)\n",
    "\n",
    "if not use_wiki:\n",
    "    df = adjust_df_labels_remove_fake(df)  \n",
    "df = LoadingAndCleaning.clean_df_only_content(df, use_multiprocessing)\n",
    "df = LoadingAndCleaning.process_df_only_content(df, use_multiprocessing)\n",
    "\n",
    "if use_wiki:\n",
    "    wiki_news = load_and_clean_wikinews()\n",
    "    df = df.append(wiki_news, ignore_index=True)\n",
    "    df = adjust_df_labels_remove_fake(df)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Lets look at few data properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>content_joined</th>\n",
       "      <th>content_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>reliable</td>\n",
       "      <td>[screenshot, youtube, mjoflakelandNUMNUM, metr...</td>\n",
       "      <td>1</td>\n",
       "      <td>screenshot youtube mjoflakelandNUMNUM metro bu...</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>reliable</td>\n",
       "      <td>[photo, reutersbrian, snyder, demonstrators, h...</td>\n",
       "      <td>1</td>\n",
       "      <td>photo reutersbrian snyder demonstrators holdin...</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>reliable</td>\n",
       "      <td>[much, fanfare, publicity, january, declared, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>much fanfare publicity january declared human ...</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        type                                            content  label  \\\n",
       "6   reliable  [screenshot, youtube, mjoflakelandNUMNUM, metr...      1   \n",
       "14  reliable  [photo, reutersbrian, snyder, demonstrators, h...      1   \n",
       "18  reliable  [much, fanfare, publicity, january, declared, ...      1   \n",
       "\n",
       "                                       content_joined  content_length  \n",
       "6   screenshot youtube mjoflakelandNUMNUM metro bu...             360  \n",
       "14  photo reutersbrian snyder demonstrators holdin...             303  \n",
       "18  much fanfare publicity january declared human ...              55  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>content_joined</th>\n",
       "      <th>content_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[free, trade, zone, antalya, turkey, NUMNUM, w...</td>\n",
       "      <td>1</td>\n",
       "      <td>free trade zone antalya turkey NUMNUM women wo...</td>\n",
       "      <td>536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[three, police, officers, NUMNUM, batalhãof, d...</td>\n",
       "      <td>1</td>\n",
       "      <td>three police officers NUMNUM batalhãof da pm m...</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[novak, story, sparked, contoversy, written, j...</td>\n",
       "      <td>1</td>\n",
       "      <td>novak story sparked contoversy written july NU...</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                            content  label  \\\n",
       "0     1  [free, trade, zone, antalya, turkey, NUMNUM, w...      1   \n",
       "1     1  [three, police, officers, NUMNUM, batalhãof, d...      1   \n",
       "2     1  [novak, story, sparked, contoversy, written, j...      1   \n",
       "\n",
       "                                      content_joined  content_length  \n",
       "0  free trade zone antalya turkey NUMNUM women wo...             536  \n",
       "1  three police officers NUMNUM batalhãof da pm m...             307  \n",
       "2  novak story sparked contoversy written july NU...             315  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if use_wiki:\n",
    "    display(wiki_news.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    29258\n",
       "0    29258\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check counts\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check length of df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58516"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of working dataframe\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define Term Frequency Inverse Document Frequency (TF IDF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tf_idf(train_df, test_df=None, use_title_and_author=False):\n",
    "    \n",
    "    if use_title_and_author:\n",
    "        feature = 'all_info'\n",
    "    else: \n",
    "        feature = 'content_joined'\n",
    "        \n",
    "    # get targe<t values\n",
    "    train_target = train_df['label'].values\n",
    "    \n",
    "    # Prepare the tf-idf (term frequency-inverse document frequency) TODO: read up on this for report\n",
    "    start_time = time.time() \n",
    "    count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "    tf_idf_transformer = TfidfTransformer(smooth_idf=False)\n",
    "\n",
    "    # fit and transform train data to count vectorizer\n",
    "    count_vectorizer.fit(train_df[feature].values)\n",
    "    count_vect_train = count_vectorizer.transform(train_df[feature].values)\n",
    "    # fit the counts vector to tfidf transformer\n",
    "    tf_idf_transformer.fit(count_vect_train)\n",
    "    tf_idf_train = tf_idf_transformer.transform(count_vect_train)\n",
    "\n",
    "    # Transform the test data as well\n",
    "    # NB! We dont use this?\n",
    "    if test_df is not None:\n",
    "        count_vect_test = count_vectorizer.transform(test_df[feature].values)\n",
    "        tf_idf_test = tf_idf_transformer.transform(count_vect_test)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Time elapsed for TF IDF transform: ,\", end_time - start_time)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(tf_idf_train, train_target, random_state=0)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, tf_idf_transformer, count_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define Classifiers for Training and Validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up model metric dataframe and model fitting\n",
    "# TODO: Training should happen in .py file, preferably pooled\n",
    "#######################\n",
    "def get_perf_metrics(model, i, X_train, y_train, X_test, y_test):\n",
    "    # model name\n",
    "    model_name = type(model).__name__\n",
    "    # time keeping\n",
    "    start_time = time.time()\n",
    "    print(\"Training {} model...\".format(model_name))\n",
    "    # Fitting of model\n",
    "    \n",
    "    # list to retain the models to use later for test set predictions\n",
    "    models_trained_list = []\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Completed {} model training.\".format(model_name))\n",
    "    elapsed_time = time.time() - start_time\n",
    "    # Time Elapsed\n",
    "    print(\"Time elapsed: {:.2f} s.\".format(elapsed_time))\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Add to ith row of dataframe - metrics\n",
    "    df_perf_metrics.loc[i] = [\n",
    "        model_name,\n",
    "        model.score(X_train, y_train),\n",
    "        model.score(X_test, y_test),\n",
    "        precision_score(y_test, y_pred),\n",
    "        recall_score(y_test, y_pred),\n",
    "        f1_score(y_test, y_pred), \"{:.2f}\".format(elapsed_time)\n",
    "    ]\n",
    "    # keep a track of trained models\n",
    "    models_trained_list.append(model)\n",
    "\n",
    "    print(\"Completed {} model's performance assessment.\".format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 4. Run the TF IDF tranforms, use default 3/4 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for TF IDF transform: , 99.30678725242615\n"
     ]
    }
   ],
   "source": [
    "# Do the tf idf transform\n",
    "# Note we save the transformer and vectorizer, because we need this to transform test data in similar way\n",
    "X_train, X_test, y_train, y_test, tf_idf_transformer, count_vectorizer = run_tf_idf(df,\n",
    "                                                                                    test_df=None,\n",
    "                                                                                    use_title_and_author=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression model...\n",
      "Completed LogisticRegression model training.\n",
      "Time elapsed: 101.88 s.\n",
      "Completed LogisticRegression model's performance assessment.\n"
     ]
    }
   ],
   "source": [
    "df_perf_metrics = pd.DataFrame(columns=[\n",
    "                'Model', 'Accuracy_Training_Set', \n",
    "                'Accuracy_Test_Set', 'Precision',\n",
    "                'Recall', 'f1_score', 'Training Time (secs)'\n",
    "                ])\n",
    "\n",
    "\n",
    "# Set up model list and run training\n",
    "#C=275, max_iter=185\n",
    "#models_list = [LogisticRegression(C=200, max_iter=200)]\n",
    "models_list = [LogisticRegression()]\n",
    "\n",
    "#models_list = [LogisticRegression(), \n",
    "#               MultinomialNB(),\n",
    "#               RandomForestClassifier(),\n",
    "#               DecisionTreeClassifier(),\n",
    "#               GradientBoostingClassifier(),\n",
    "#               AdaBoostClassifier()]\n",
    "\n",
    "train_models = True\n",
    "if train_models: \n",
    "    for n, model in enumerate(models_list):\n",
    "        get_perf_metrics(model, n, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy_Training_Set</th>\n",
       "      <th>Accuracy_Test_Set</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>Training Time (secs)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.944972</td>\n",
       "      <td>0.839087</td>\n",
       "      <td>0.812244</td>\n",
       "      <td>0.877178</td>\n",
       "      <td>0.843463</td>\n",
       "      <td>101.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model  Accuracy_Training_Set  Accuracy_Test_Set  Precision  \\\n",
       "0  LogisticRegression               0.944972           0.839087   0.812244   \n",
       "\n",
       "     Recall  f1_score Training Time (secs)  \n",
       "0  0.877178  0.843463               101.88  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at metrics for all models\n",
    "df_perf_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Consider saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_model(pkl_filename):\n",
    "    import pickle\n",
    "    # Save to file in the current working directory\n",
    "    # The model: \"models/log_reg_optimized_100percent_data.pkl\" has the following params:\n",
    "    # 100% data (394k rows after cleaning), C: 150, max iter 200, added title and author\n",
    "    # tested on political/reliable split, balanced by dropping nans\n",
    "    #pkl_filename = \"models/base_model_not_dropped_nans.pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(pkl_filename):\n",
    "    with open(pkl_filename, 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Prepare and test on LIAR\n",
    "We map true and mostly true to \"real\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_liar():\n",
    "    liar_path = \"data/liar_dataset/test.tsv\"\n",
    "    liar_df = pd.read_csv(liar_path, sep='\\t', names=[\"ID\", \"label\", \"statement\", \"subject\", \"speaker\", \"job title\", \"state \", \"party \",\"total credit\", \"barely true counts\",\"false counts\", \"half true counts\", \"mostly true counts\", \"pants on fire counts\",\"context \"])\n",
    "    liar_df['custom_label'] = np.where(((liar_df['label'] == 'true') | (liar_df['label'] == 'mostly-true')), 1, 0)\n",
    "    liar_df = liar_df.dropna(subset=['label', 'statement'])\n",
    "    liar_df['statement'] = liar_df['statement'].str.strip()\n",
    "    liar_df = liar_df.drop_duplicates(subset=\"statement\") \n",
    "    liar_df[\"statement\"] = pd.concat(map(LoadingAndCleaning.clean_content, np.array_split(liar_df[\"statement\"], 32)))\n",
    "    liar_df[\"statement\"] = pd.concat(map(LoadingAndCleaning.clean_content, np.array_split(liar_df[\"statement\"], 32)))\n",
    "    liar_df[\"statement\"] = pd.concat(map(LoadingAndCleaning.tokenize, np.array_split(liar_df[\"statement\"], 32)))\n",
    "    liar_df[\"statement\"] = pd.concat(map(LoadingAndCleaning.remove_stopwords, np.array_split(liar_df[\"statement\"], 32)))\n",
    "    liar_df[\"statement_joined\"] = pd.concat(map(LoadingAndCleaning.recombine_content, np.array_split(liar_df[\"statement\"], 32)))\n",
    "    liar_df[\"statement_len\"] = pd.concat(map(LoadingAndCleaning.add_content_length, np.array_split(liar_df[\"statement\"], 32)))\n",
    "    return liar_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tf_idf_for_test_prep(df, tf_idf_transformer, count_vectorizer, feature, target = False):\n",
    "\n",
    "    # We only have target for LIAR, not kaggle\n",
    "    if target:\n",
    "        test_target = liar_df['custom_label'].values\n",
    "    \n",
    "    # Prepare the tf-idf (term frequency-inverse document frequency) TODO: read up on this for report\n",
    "    start_time = time.time()\n",
    "    \n",
    "    count_vect_test = count_vectorizer.transform(df[feature].values)\n",
    "    tf_idf_test = tf_idf_transformer.transform(count_vect_test)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Time elapsed for TF IDF transform: ,\", end_time - start_time)\n",
    "\n",
    "    if target:\n",
    "        return tf_idf_test, test_target\n",
    "    else:\n",
    "        return tf_idf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_df = prepare_liar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.26440410418311"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liar_df['statement_len'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>job title</th>\n",
       "      <th>state</th>\n",
       "      <th>party</th>\n",
       "      <th>total credit</th>\n",
       "      <th>barely true counts</th>\n",
       "      <th>false counts</th>\n",
       "      <th>half true counts</th>\n",
       "      <th>mostly true counts</th>\n",
       "      <th>pants on fire counts</th>\n",
       "      <th>context</th>\n",
       "      <th>custom_label</th>\n",
       "      <th>statement_joined</th>\n",
       "      <th>statement_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11972.json</td>\n",
       "      <td>true</td>\n",
       "      <td>[building, wall, yousmexico, border, take, lit...</td>\n",
       "      <td>immigration</td>\n",
       "      <td>rick-perry</td>\n",
       "      <td>Governor</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>42</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>Radio interview</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>building wall yousmexico border take literally...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11685.json</td>\n",
       "      <td>false</td>\n",
       "      <td>[wisconsin, pace, double, number, layoffs, year]</td>\n",
       "      <td>jobs</td>\n",
       "      <td>katrina-shankland</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>democrat</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a news conference</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>wisconsin pace double number layoffs year</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  label                                          statement  \\\n",
       "0  11972.json   true  [building, wall, yousmexico, border, take, lit...   \n",
       "1  11685.json  false   [wisconsin, pace, double, number, layoffs, year]   \n",
       "\n",
       "       subject            speaker             job title     state   \\\n",
       "0  immigration         rick-perry              Governor      Texas   \n",
       "1         jobs  katrina-shankland  State representative  Wisconsin   \n",
       "\n",
       "       party   total credit  barely true counts  false counts  \\\n",
       "0  republican            30                  30            42   \n",
       "1    democrat             2                   1             0   \n",
       "\n",
       "   half true counts  mostly true counts pants on fire counts  context   \\\n",
       "0                23                  18      Radio interview       NaN   \n",
       "1                 0                   0    a news conference       NaN   \n",
       "\n",
       "   custom_label                                   statement_joined  \\\n",
       "0             1  building wall yousmexico border take literally...   \n",
       "1             0          wisconsin pace double number layoffs year   \n",
       "\n",
       "   statement_len  \n",
       "0              7  \n",
       "1              6  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liar_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Run the TF IDF transform on LIAR ste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for TF IDF transform: , 0.14763641357421875\n"
     ]
    }
   ],
   "source": [
    "LIAR_tf_idf_test, LIAR_train_target = run_tf_idf_for_test_prep(liar_df,\n",
    "                                                               tf_idf_transformer,\n",
    "                                                               count_vectorizer,\n",
    "                                                               feature = \"statement_joined\",\n",
    "                                                               target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIAR_predictions = model.predict(LIAR_tf_idf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    50.0\n",
      "0    50.0\n",
      "Name: label, dtype: float64\n",
      "[16.96921863 83.03078137]\n"
     ]
    }
   ],
   "source": [
    "# Real distribution\n",
    "print(df['label'].value_counts() * 100/len(df))\n",
    "\n",
    "# Now print our predicted distribution\n",
    "print(np.bincount(LIAR_predictions)* 100 / len(LIAR_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Look at LIAR preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3898973954222573\n",
      "Precision:  0.34600760456273766\n",
      "Recall:  0.8106904231625836\n",
      "f1-score:  0.4850099933377749\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', accuracy_score(LIAR_train_target, LIAR_predictions))\n",
    "print('Precision: ', precision_score(LIAR_train_target, LIAR_predictions))\n",
    "print('Recall: ', recall_score(LIAR_train_target, LIAR_predictions))\n",
    "print('f1-score: ', f1_score(LIAR_train_target, LIAR_predictions))\n",
    "#fuck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Prepare Kaggle set for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjustable cleaner\n",
    "def custom_clean(df, feature):\n",
    "    feature_joined = feature + \"_joined\"\n",
    "    df[feature] = pd.concat(map(LoadingAndCleaning.clean_content, np.array_split(df[feature], 32)))\n",
    "    df[feature] = pd.concat(map(LoadingAndCleaning.tokenize, np.array_split(df[feature], 32)))\n",
    "    df[feature] = pd.concat(map(LoadingAndCleaning.remove_stopwords, np.array_split(df[feature], 32)))\n",
    "    df[feature_joined] = pd.concat(map(LoadingAndCleaning.recombine_content, np.array_split(df[feature], 32)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_kaggle():\n",
    "    kaggle_path = \"data/kaggle_test_data/test_set.json\"\n",
    "    kaggle_df = pd.read_json(kaggle_path)\n",
    "    kaggle_df = custom_clean(kaggle_df, \"article\")\n",
    "    return kaggle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df = prepare_kaggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article</th>\n",
       "      <th>article_joined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>[daniel, greenfield, shillman, journalism, fel...</td>\n",
       "      <td>daniel greenfield shillman journalism fellow f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>[google, pinterest, digg, linkedin, reddit, st...</td>\n",
       "      <td>google pinterest digg linkedin reddit stumbleu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>[yous, secretary, state, john, f, kerry, said,...</td>\n",
       "      <td>yous secretary state john f kerry said monday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>[kaydee, king, kaydeeking, november, NUMNUM, N...</td>\n",
       "      <td>kaydee king kaydeeking november NUMNUM NUMNUM ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>[primary, day, new, york, frontrunners, hillar...</td>\n",
       "      <td>primary day new york frontrunners hillary clin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                            article  \\\n",
       "0   8476  [daniel, greenfield, shillman, journalism, fel...   \n",
       "1  10294  [google, pinterest, digg, linkedin, reddit, st...   \n",
       "2   3608  [yous, secretary, state, john, f, kerry, said,...   \n",
       "3  10142  [kaydee, king, kaydeeking, november, NUMNUM, N...   \n",
       "4    875  [primary, day, new, york, frontrunners, hillar...   \n",
       "\n",
       "                                      article_joined  \n",
       "0  daniel greenfield shillman journalism fellow f...  \n",
       "1  google pinterest digg linkedin reddit stumbleu...  \n",
       "2  yous secretary state john f kerry said monday ...  \n",
       "3  kaydee king kaydeeking november NUMNUM NUMNUM ...  \n",
       "4  primary day new york frontrunners hillary clin...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 TF IDF Transform Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for TF IDF transform: , 5.432998180389404\n"
     ]
    }
   ],
   "source": [
    "kaggle_tf_idf_test = run_tf_idf_for_test_prep(kaggle_df, tf_idf_transformer, count_vectorizer, feature='article_joined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Get predictions and print scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_predictions = model.predict(kaggle_tf_idf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    50.0\n",
      "0    50.0\n",
      "Name: label, dtype: float64\n",
      "Our predicted distribution of Kaggle's real/fake:  [30.86029992 69.13970008]\n"
     ]
    }
   ],
   "source": [
    "# Real distribution\n",
    "print(df['label'].value_counts() * 100/len(df))\n",
    "\n",
    "# Now print our predicted distribution\n",
    "\n",
    "print(\"Our predicted distribution of Kaggle's real/fake: \", np.bincount(kaggle_predictions)* 100 / len(kaggle_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kaggle_submission(kaggle_df, kaggle_predictions):\n",
    "    kaggle_submission = kaggle_df\n",
    "    kaggle_submission['label'] = kaggle_predictions.tolist()\n",
    "    kaggle_submission = kaggle_submission.drop(['article', 'article_joined'], axis=1)\n",
    "    kaggle_submission['label'] = np.where(((kaggle_submission['label'] == 1)), \"REAL\", \"FAKE\")\n",
    "    return kaggle_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_submission = get_kaggle_submission(kaggle_df, kaggle_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id label\n",
       "0   8476  REAL\n",
       "1  10294  REAL\n",
       "2   3608  REAL\n",
       "3  10142  REAL\n",
       "4    875  REAL"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.4 Save kaggle preds into submission folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_kaggle_submission:\n",
    "    #kaggle_path = 'data/submissions/base_model_preds.csv'\n",
    "    kaggle_submission.to_csv(kaggle_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Tuning logistic regression C and max iter (Found C: 275, max iter: 185 optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#max_iter = [200, 250, 300]\n",
    "#C = [75, 100, 125, 150]\n",
    "# Optimal C: 150, max iter 200\n",
    "\n",
    "# So, we found the lowest max iter, and highest C to be optimal, and can retune with lower/higher values\n",
    "# Problem, we get this max iter error when using lower max iter... But we try retuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_iter = [175, 185, 200]\n",
    "#C = [150, 175, 200]\n",
    "\n",
    "#Retuning params, we got optimal max iter: 185, and C: 200\n",
    "#So, we try using max iter: 175, and then bigger C values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_iter = [185]\n",
    "#C = [200, 250, 275]\n",
    "\n",
    "# With these params, we got an optimal of 185 max iter, and 275 C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_iter = [200]\n",
    "#C = [275]\n",
    "\n",
    "#param_grid = dict(max_iter=max_iter, C=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch_C_and_maxiter(C, max_iter):\n",
    "#max_iter = [200]\n",
    "#C = [275]\n",
    "    model = LogisticRegression()\n",
    "    param_grid = dict(max_iter=max_iter, C=C)\n",
    "    grid = GridSearchCV(estimator=model,\n",
    "                        param_grid=param_grid,\n",
    "                        cv=5,\n",
    "                        scoring=['f1'],\n",
    "                        refit='f1',\n",
    "                        verbose=2)\n",
    "    \n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "    \n",
    "    return grid_result\n",
    "    \n",
    "    #best_model = grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_iter = [200]\n",
    "#C = [275]\n",
    "# Run gridsearch:\n",
    "#grid_result = gridsearch_C_and_maxiter(C, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Print performance of best C and max iter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best C and max iter vals\n",
    "#print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_performance(grid_result):\n",
    "    model = grid_result.best_estimator_\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print('Precision: ', precision_score(y_test, y_pred))\n",
    "    print('Recall: ', recall_score(y_test, y_pred))\n",
    "    print('f1-score: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_performance(grid_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
